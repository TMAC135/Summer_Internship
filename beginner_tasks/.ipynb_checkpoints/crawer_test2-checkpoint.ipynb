{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named bs4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e4ffac3242cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##爬取sohu新闻的例子\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhttplib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named bs4"
     ]
    }
   ],
   "source": [
    "##爬取sohu新闻的例子,使用urllib2包\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import httplib\n",
    "\n",
    "\n",
    "class Spider(object):\n",
    "    \"\"\"Spider\"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def getNextUrls(self):\n",
    "        urls = []\n",
    "        request = urllib2.Request(self.url)\n",
    "        request.add_header('User-Agent','Mozilla/5.0 (Windows NT 6.1; \\\n",
    "            WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36')\n",
    "        try:\n",
    "            html = urllib2.urlopen(request)\n",
    "        except socket.timeout, e:\n",
    "            pass\n",
    "        except urllib2.URLError,ee:\n",
    "            pass\n",
    "        except httplib.BadStatusLine:\n",
    "            pass\n",
    "\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            print(\"http://m.sohu.com\" + link.get('href'))\n",
    "            if link.get('href')[0] == '/':\n",
    "                urls.append(\"http://m.sohu.com\" + link.get('href'))\n",
    "        return urls\n",
    "\n",
    "def getNews(url):\n",
    "    print url\n",
    "    xinwen = ''\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('User-Agent','Mozilla/5.0 (Windows NT 6.1; \\\n",
    "        WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36')\n",
    "    try:\n",
    "        html = urllib2.urlopen(request)\n",
    "    except urllib2.HTTPError, e:\n",
    "        print e.code\n",
    "\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    for news in soup.select('p.para'):\n",
    "        xinwen += news.get_text().decode('utf-8')\n",
    "    return xinwen\n",
    "\n",
    "\n",
    "class News(object):\n",
    "    \"\"\"\n",
    "    source:from where 从哪里爬取的网站\n",
    "    title:title of news  文章的标题    \n",
    "    time:published time of news 文章发布时间\n",
    "    content:content of news 文章内容\n",
    "    type:type of news    文章类型\n",
    "    \"\"\"\n",
    "    def __init__(self, source, title, time, content, type):\n",
    "        self.source = source              \n",
    "        self.title = title                 \n",
    "        self.time = time                \n",
    "        self.content = content            \n",
    "        self.type = type                \n",
    "\n",
    "\n",
    "file = open('C:/test.txt','a')\n",
    "for i in range(38,50):\n",
    "    for j in range(1,5):\n",
    "        url = \"http://m.sohu.com/cr/\" + str(i) + \"/?page=\" + str(j)\n",
    "        print url\n",
    "        s = Spider(url)\n",
    "        for newsUrl in s.getNextUrls():\n",
    "            file.write(getNews(newsUrl))\n",
    "            file.write(\"\\n\")\n",
    "            print \"---------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'http://m.sohu.com/', u'http://m.sohu.com/', u'http://m.sohu.com/n/411390673/', u'http://m.sohu.com/n/411147437/', u'http://m.sohu.com/n/556347094/', u'http://m.sohu.com/n/410604959/', u'http://m.sohu.com/n/556344787/', u'http://m.sohu.com/n/407406297/', u'http://m.sohu.com/n/556334011/', u'http://m.sohu.com/n/556333169/', u'http://m.sohu.com/n/556330356/', u'http://m.sohu.com/n/556330352/', u'http://m.sohu.com/n/556328581/', u'http://m.sohu.com/n/556327189/', u'http://m.sohu.com/n/556327180/', u'http://m.sohu.com/n/404889160/', u'http://m.sohu.com/n/556324200/', u'http://m.sohu.com/', u'http://m.sohu.com/c/2/', u'http://m.sohu.com/c/3/', u'http://m.sohu.com/c/4/', u'http://m.sohu.com/c/395/', u'http://m.sohu.com/c/3804/?_once_=000058_v3', u'http://m.sohu.com/help/', u'http://m.sohu.com/c/432/']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'http://m.sohu.com/',\n",
       " u'http://m.sohu.com/',\n",
       " u'http://m.sohu.com/n/411390673/',\n",
       " u'http://m.sohu.com/n/411147437/',\n",
       " u'http://m.sohu.com/n/556347094/',\n",
       " u'http://m.sohu.com/n/410604959/',\n",
       " u'http://m.sohu.com/n/556344787/',\n",
       " u'http://m.sohu.com/n/407406297/',\n",
       " u'http://m.sohu.com/n/556334011/',\n",
       " u'http://m.sohu.com/n/556333169/',\n",
       " u'http://m.sohu.com/n/556330356/',\n",
       " u'http://m.sohu.com/n/556330352/',\n",
       " u'http://m.sohu.com/n/556328581/',\n",
       " u'http://m.sohu.com/n/556327189/',\n",
       " u'http://m.sohu.com/n/556327180/',\n",
       " u'http://m.sohu.com/n/404889160/',\n",
       " u'http://m.sohu.com/n/556324200/',\n",
       " u'http://m.sohu.com/',\n",
       " u'http://m.sohu.com/c/2/',\n",
       " u'http://m.sohu.com/c/3/',\n",
       " u'http://m.sohu.com/c/4/',\n",
       " u'http://m.sohu.com/c/395/',\n",
       " u'http://m.sohu.com/c/3804/?_once_=000058_v3',\n",
       " u'http://m.sohu.com/help/',\n",
       " u'http://m.sohu.com/c/432/']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Spider(object):\n",
    "    \"\"\"Spider\"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.html = None\n",
    "        self.urls = None\n",
    "        \n",
    "    def generateHtml(self):\n",
    "        '''\n",
    "        get the html strings by request\n",
    "        '''\n",
    "        request = urllib2.Request(self.url)\n",
    "        request.add_header('User-Agent','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36')\n",
    "        try:\n",
    "            html = urllib2.urlopen(request)\n",
    "            self.html = html\n",
    "        except socket.timeout, e:\n",
    "            print e.reason\n",
    "        except urllib2.URLError,ee:\n",
    "            print e.reason\n",
    "        except httplib.BadStatusLine:\n",
    "            print e.reason\n",
    "        \n",
    "        \n",
    "    def getAlltUrls(self):\n",
    "        '''\n",
    "        get all URLS from the <a> in the page\n",
    "        '''\n",
    "        if not self.urls:      \n",
    "            urls = []\n",
    "            if not self.html:\n",
    "                return\n",
    "            else:\n",
    "                soup = BeautifulSoup(self.html)\n",
    "                for link in soup.find_all('a'):\n",
    "        #             print(\"http://m.sohu.com\" + link.get('href'))\n",
    "                    if link.get('href')[0] == '/':\n",
    "                        urls.append(\"http://m.sohu.com\" + link.get('href'))\n",
    "                self.urls = urls\n",
    "                print urls\n",
    "    #             print len(urls)\n",
    "#                 return urls\n",
    "\n",
    "    \n",
    "    def getStrings(self):\n",
    "        '''\n",
    "        get the string representations of the news contends\n",
    "        '''\n",
    "        s_all = []\n",
    "        if not self.urls:\n",
    "            return\n",
    "        else:\n",
    "#             print len(self.urls)\n",
    "            for url in self.urls:\n",
    "#                 html = \n",
    "                soup = BeautifulSoup()\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "    \n",
    "s = Spider('http://m.sohu.com/cr/1/?page=2')\n",
    "s.generateHtml()\n",
    "s.getAlltUrls()\n",
    "s.getStrings()\n",
    "s.urls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<addinfourl at 4371478000 whose fp = <socket._fileobject object at 0x1052799d0>>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10月22日下午6点30分，身穿灰色短袖T恤、蓝色牛仔裤的（FB.O）公司创始人兼首席执行官马克•扎克伯格（MarkZuckerberg）出现在了清华经管学院舜德楼，并与主持人进行了一场全程中文的对话。\n",
      "在对话中，他提到了自己从2010年开始学中文。原因是他想和太太普里西拉•陈(Priscilla Chan)以及太太的奶奶交流。另外，他认为中国是特别伟大的国家，因此想通过学中文感受中国魅力。他还说，普通话非常难学，所以想挑战自己。\n",
      "认为中国有很多优秀创业公司\n",
      "谈及中国的创新问题时，扎克伯格认为中国有很多创新公司。\n",
      "扎克伯格透露，10月21日他与小米公司CEO雷军吃饭。他认为，小米有很好的产品，而且便宜，相信会发展很快。此外，他认为腾讯的微信业务做得很大，淘宝则创造了工作机会。\n",
      "他告诉在场学生，最好的公司都不是因为创始人想要创业而被创立的，而是因为创始人想要改变世界。\n",
      "启动在华招聘 将帮助中国企业增加外国客户\n",
      "扎克伯格透露，Facebook明年将在中国展开招聘，上个月已经招聘了20名中国学生。扎克伯格称，公司在中国帮助中国公司增长国外客户，如中国公司可以用Facebook广告连接世界。他举例，lenovo已经在印尼用我们的广告卖新的手机。\n",
      "另外，他称自己希望想要帮助世界其他国家去连接中国。目前，杭州和青岛也有Facebook页面。“我们和城市合作发展页面，分享中国文化。”\n",
      "Facebook未来十年将发展人工智能和虚拟现实\n",
      "“我们最大的挑战发生在2012年。我们要把Facebook变成移动互联网公司”，扎克伯格回忆，2012年公司收入增长很慢，但当Facebook变成移动公司后，现在有了7亿移动用户。\n",
      "今年是Facebook十周年生日，扎克伯格说下一个十年将发展三点：第一是连接整个世界；第二是发展人工智能；第三是发展虚拟现实（virtual reality）。他透露，Oculus将是第一产品。\n",
      "出任清华经管顾问委员会委员\n",
      "清华大学经济管理学院对腾讯财经表示，近日该学院公布了学院顾问委员会2014-2015学年委员名单。今年学院顾问委员会新增委员3人，新增接任委员5人。除了Facebook创始人兼首席执行官马克•扎克伯格（Mark Zuckerberg），还有2位新增委员分别是百威英博首席执行官薄睿拓（Carlos Brito），公司董事会主席、总裁兼首席执行官罗睿兰（Ginni Rometty）。\n",
      "对于出任清华经管学院顾问委员会委员，扎克伯克表示原因是自己非常关心教育。“我在美国做了很多支持教育的事情，希望参加清华经管顾问委员会能为我提供一个好机会，学习和支持中国的教育”。\n"
     ]
    }
   ],
   "source": [
    "## debug: extract all the unicode strings for a specific sohu news\n",
    "s = 'http://m.sohu.com/n/556327189/'\n",
    "request = urllib2.Request(s)\n",
    "#set the contends of the header\n",
    "request.add_header('User-Agent','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36')\n",
    "try:\n",
    "    html = urllib2.urlopen(request)\n",
    "except socket.timeout, e:\n",
    "    print e.reason\n",
    "except urllib2.URLError,ee:\n",
    "    print e.reason\n",
    "except httplib.BadStatusLine:\n",
    "    print e.reason\n",
    "soup = BeautifulSoup(html)\n",
    "# print soup.find_all('p',class_='para')\n",
    "\n",
    "# 将para 的\n",
    "for s in soup.find_all('p',class_='para'):\n",
    "#     print s.strings,'\\n'\n",
    "    tmp = s.strings\n",
    "    for i in tmp:\n",
    "        print i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
