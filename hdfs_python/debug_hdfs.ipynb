{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named hfile",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0cd17fea4d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named hfile"
     ]
    }
   ],
   "source": [
    "from hdfs.hfile import Hfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>hdfs 的外部包的使用 - hdfs.hfile</h1>\n",
       "<a href='https://github.com/traviscrawford/python-hdfs'>link</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h1>hdfs 的外部包的使用 - hdfs.hfile</h1>\n",
    "<a href='https://github.com/traviscrawford/python-hdfs'>link</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Hfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0665835338b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Let's open local and HDFS files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mhfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhostname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Hfile' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "hostname = 'hadoop.twitter.com'\n",
    "port = 8020\n",
    "hdfs_path = '/user/travis/example'\n",
    "local_path = '/etc/motd'\n",
    "\n",
    "# Let's open local and HDFS files.\n",
    "\n",
    "hfile = Hfile(hostname, port, hdfs_path, mode='w')\n",
    "fh = open(local_path)\n",
    "\n",
    "# Now we'll write lines from a local file into the HDFS file.\n",
    "for line in fh:\n",
    "  hfile.write(line)\n",
    "\n",
    "# And close them.\n",
    "fh.close()\n",
    "hfile.close()\n",
    "\n",
    "# Let's read local_path into memory for comparison.\n",
    "motd = open(local_path).read()\n",
    "\n",
    "# Now let's read the data back\n",
    "hfile = Hfile(hostname, port, hdfs_path)\n",
    "\n",
    "# With an iterator\n",
    "data_read_from_hdfs = ''\n",
    "for line in hfile:\n",
    "  data_read_from_hdfs += line\n",
    "print motd == data_read_from_hdfs\n",
    "\n",
    "# All at once\n",
    "data_read_from_hdfs = hfile.read()\n",
    "print motd == data_read_from_hdfs\n",
    "\n",
    "hfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>hdfs 包的debug - hdfs.Client(InsecureClient)</h1>\n",
       "<a href = \"http://hdfscli.readthedocs.io/en/latest/api.html\"></a>\n",
       "<a href=\"https://media.readthedocs.org/pdf/hdfscli/latest/hdfscli.pdf\">User Guide(with interactive shell)</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h1>hdfs 包的debug - hdfs.Client(InsecureClient)</h1>\n",
    "<a href = \"http://hdfscli.readthedocs.io/en/latest/api.html\"></a>\n",
    "<a href=\"https://media.readthedocs.org/pdf/hdfscli/latest/hdfscli.pdf\">User Guide(with interactive shell)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'spaceConsumed': 1524481883764, u'quota': 9223372036854775807, u'spaceQuota': -1, u'length': 262537341708, u'directoryCount': 223, u'fileCount': 113601}\n",
      "[u'test.txt', u'test2.txt']\n"
     ]
    }
   ],
   "source": [
    "import hdfs\n",
    "from hdfs import InsecureClient\n",
    "client = InsecureClient('http://192.168.121.2:50070')\n",
    "\n",
    "print client.content('/')\n",
    "print client.list('/demo')\n",
    "\n",
    "# 读文件\n",
    "# with client.read('/demo/test.txt') as reader:\n",
    "#   features = reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__registry__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_append', '_create', '_delete', '_get_content_summary', '_get_file_checksum', '_get_file_status', '_get_home_directory', '_list_status', '_mkdirs', '_open', '_rename', '_request', '_session', '_set_owner', '_set_permission', '_set_replication', '_set_times', '_timeout', 'checksum', 'content', 'delete', 'download', 'from_options', 'list', 'makedirs', 'parts', 'read', 'rename', 'resolve', 'root', 'set_owner', 'set_permission', 'set_replication', 'set_times', 'status', 'upload', 'url', 'walk', 'write']\n",
      "Help on InsecureClient in module hdfs.client object:\n",
      "\n",
      "class InsecureClient(Client)\n",
      " |  HDFS web client to use when security is off.\n",
      " |  \n",
      " |  :param url: Hostname or IP address of HDFS namenode, prefixed with protocol,\n",
      " |    followed by WebHDFS port on namenode\n",
      " |  :param user: User default. Defaults to the current user's (as determined by\n",
      " |    `whoami`).\n",
      " |  :param \\*\\*kwargs: Keyword arguments passed to the base class' constructor.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      InsecureClient\n",
      " |      Client\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, url, user=None, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Client:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  checksum(self, hdfs_path)\n",
      " |      Get a remote file's checksum.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path. Must point to a file.\n",
      " |  \n",
      " |  content(self, hdfs_path, strict=True)\n",
      " |      Get ContentSummary_ for a file or folder on HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path.\n",
      " |      :param strict: If `False`, return `None` rather than raise an exception if\n",
      " |        the path doesn't exist.\n",
      " |      \n",
      " |      .. _ContentSummary: CS_\n",
      " |      .. _CS: http://hadoop.apache.org/docs/r1.0.4/webhdfs.html#ContentSummary\n",
      " |  \n",
      " |  delete(self, hdfs_path, recursive=False)\n",
      " |      Remove a file or directory from HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param recursive: Recursively delete files and directories. By default,\n",
      " |        this method will raise an :class:`HdfsError` if trying to delete a\n",
      " |        non-empty directory.\n",
      " |      \n",
      " |      This function returns `True` if the deletion was successful and `False` if\n",
      " |      no file or directory previously existed at `hdfs_path`.\n",
      " |  \n",
      " |  download(self, hdfs_path, local_path, overwrite=False, n_threads=1, temp_dir=None, **kwargs)\n",
      " |      Download a file or folder from HDFS and save it locally.\n",
      " |      \n",
      " |      :param hdfs_path: Path on HDFS of the file or folder to download. If a\n",
      " |        folder, all the files under it will be downloaded.\n",
      " |      :param local_path: Local path. If it already exists and is a directory,\n",
      " |        the files will be downloaded inside of it.\n",
      " |      :param overwrite: Overwrite any existing file or directory.\n",
      " |      :param n_threads: Number of threads to use for parallelization. A value of\n",
      " |        `0` (or negative) uses as many threads as there are files.\n",
      " |      :param temp_dir: Directory under which the files will first be downloaded\n",
      " |        when `overwrite=True` and the final destination path already exists. Once\n",
      " |        the download successfully completes, it will be swapped in.\n",
      " |      :param \\*\\*kwargs: Keyword arguments forwarded to :meth:`read`. If no\n",
      " |        `chunk_size` argument is passed, a default value of 64 kB will be used.\n",
      " |        If a `progress` argument is passed and threading is used, care must be\n",
      " |        taken to ensure correct behavior.\n",
      " |      \n",
      " |      On success, this method returns the local download path.\n",
      " |  \n",
      " |  list(self, hdfs_path, status=False)\n",
      " |      Return names of files contained in a remote folder.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path to a directory. If `hdfs_path` doesn't exist\n",
      " |        or points to a normal file, an :class:`HdfsError` will be raised.\n",
      " |      :param status: Also return each file's corresponding FileStatus_.\n",
      " |  \n",
      " |  makedirs(self, hdfs_path, permission=None)\n",
      " |      Create a remote directory, recursively if necessary.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path. Intermediate directories will be created\n",
      " |        appropriately.\n",
      " |      :param permission: Octal permission to set on the newly created directory.\n",
      " |        These permissions will only be set on directories that do not already\n",
      " |        exist.\n",
      " |      \n",
      " |      This function currently has no return value as WebHDFS doesn't return a\n",
      " |      meaningful flag.\n",
      " |  \n",
      " |  parts(self, hdfs_path, parts=None, status=False)\n",
      " |      Returns a dictionary of part-files corresponding to a path.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path.\n",
      " |      :param parts: List of part-files numbers or total number of part-files to\n",
      " |        select. If a number, that many partitions will be chosen at random. By\n",
      " |        default all part-files are returned. If `parts` is a list and one of the\n",
      " |        parts is not found or too many samples are demanded, an\n",
      " |        :class:`~hdfs.util.HdfsError` is raised.\n",
      " |      :param status: Also return each file's corresponding FileStatus_.\n",
      " |  \n",
      " |  read(*args, **kwds)\n",
      " |      Read a file from HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param offset: Starting byte position.\n",
      " |      :param length: Number of bytes to be processed. `None` will read the entire\n",
      " |        file.\n",
      " |      :param buffer_size: Size of the buffer in bytes used for transferring the\n",
      " |        data. Defaults the the value set in the HDFS configuration.\n",
      " |      :param encoding: Encoding used to decode the request. By default the raw\n",
      " |        data is returned. This is mostly helpful in python 3, for example to\n",
      " |        deserialize JSON data (as the decoder expects unicode).\n",
      " |      :param chunk_size: If set to a positive number, the context manager will\n",
      " |        return a generator yielding every `chunk_size` bytes instead of a\n",
      " |        file-like object (unless `delimiter` is also set, see below).\n",
      " |      :param delimiter: If set, the context manager will return a generator\n",
      " |        yielding each time the delimiter is encountered. This parameter requires\n",
      " |        the `encoding` to be specified.\n",
      " |      :param progress: Callback function to track progress, called every\n",
      " |        `chunk_size` bytes (not available if the chunk size isn't specified). It\n",
      " |        will be passed two arguments, the path to the file being uploaded and the\n",
      " |        number of bytes transferred so far. On completion, it will be called once\n",
      " |        with `-1` as second argument.\n",
      " |      \n",
      " |      This method must be called using a `with` block:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        with client.read('foo') as reader:\n",
      " |          content = reader.read()\n",
      " |      \n",
      " |      This ensures that connections are always properly closed.\n",
      " |  \n",
      " |  rename(self, hdfs_src_path, hdfs_dst_path)\n",
      " |      Move a file or folder.\n",
      " |      \n",
      " |      :param hdfs_src_path: Source path.\n",
      " |      :param hdfs_dst_path: Destination path. If the path already exists and is\n",
      " |        a directory, the source will be moved into it. If the path exists and is\n",
      " |        a file, or if a parent destination directory is missing, this method will\n",
      " |        raise an :class:`HdfsError`.\n",
      " |  \n",
      " |  resolve(self, hdfs_path)\n",
      " |      Return absolute, normalized path, with special markers expanded.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path.\n",
      " |      \n",
      " |      Currently supported markers:\n",
      " |      \n",
      " |      * `'#LATEST'`: this marker gets expanded to the most recently updated file\n",
      " |        or folder. They can be combined using the `'{N}'` suffix. For example,\n",
      " |        `'foo/#LATEST{2}'` is equivalent to `'foo/#LATEST/#LATEST'`.\n",
      " |  \n",
      " |  set_owner(self, hdfs_path, owner=None, group=None)\n",
      " |      Change the owner of file.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param owner: Optional, new owner for file.\n",
      " |      :param group: Optional, new group for file.\n",
      " |      \n",
      " |      At least one of `owner` and `group` must be specified.\n",
      " |  \n",
      " |  set_permission(self, hdfs_path, permission)\n",
      " |      Change the permissions of file.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param permission: New octal permissions string of file.\n",
      " |  \n",
      " |  set_replication(self, hdfs_path, replication)\n",
      " |      Set file replication.\n",
      " |      \n",
      " |      :param hdfs_path: Path to an existing remote file. An :class:`HdfsError`\n",
      " |        will be raised if the path doesn't exist or points to a directory.\n",
      " |      :param replication: Replication factor.\n",
      " |  \n",
      " |  set_times(self, hdfs_path, access_time=None, modification_time=None)\n",
      " |      Change remote timestamps.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param access_time: Timestamp of last file access.\n",
      " |      :param modification_time: Timestamps of last file access.\n",
      " |  \n",
      " |  status(self, hdfs_path, strict=True)\n",
      " |      Get FileStatus_ for a file or folder on HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path.\n",
      " |      :param strict: If `False`, return `None` rather than raise an exception if\n",
      " |        the path doesn't exist.\n",
      " |      \n",
      " |      .. _FileStatus: FS_\n",
      " |      .. _FS: http://hadoop.apache.org/docs/r1.0.4/webhdfs.html#FileStatus\n",
      " |  \n",
      " |  upload(self, hdfs_path, local_path, overwrite=False, n_threads=1, temp_dir=None, chunk_size=65536, progress=None, cleanup=True, **kwargs)\n",
      " |      Upload a file or directory to HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: Target HDFS path. If it already exists and is a\n",
      " |        directory, files will be uploaded inside.\n",
      " |      :param local_path: Local path to file or folder. If a folder, all the files\n",
      " |        inside of it will be uploaded (note that this implies that folders empty\n",
      " |        of files will not be created remotely).\n",
      " |      :param overwrite: Overwrite any existing file or directory.\n",
      " |      :param n_threads: Number of threads to use for parallelization. A value of\n",
      " |        `0` (or negative) uses as many threads as there are files.\n",
      " |      :param temp_dir: Directory under which the files will first be uploaded\n",
      " |        when `overwrite=True` and the final remote path already exists. Once the\n",
      " |        upload successfully completes, it will be swapped in.\n",
      " |      :param chunk_size: Interval in bytes by which the files will be uploaded.\n",
      " |      :param progress: Callback function to track progress, called every\n",
      " |        `chunk_size` bytes. It will be passed two arguments, the path to the\n",
      " |        file being uploaded and the number of bytes transferred so far. On\n",
      " |        completion, it will be called once with `-1` as second argument.\n",
      " |      :param cleanup: Delete any uploaded files if an error occurs during the\n",
      " |        upload.\n",
      " |      :param \\*\\*kwargs: Keyword arguments forwarded to :meth:`write`.\n",
      " |      \n",
      " |      On success, this method returns the remote upload path.\n",
      " |  \n",
      " |  walk(self, hdfs_path, depth=0, status=False)\n",
      " |      Depth-first walk of remote filesystem.\n",
      " |      \n",
      " |      :param hdfs_path: Starting path. If the path doesn't exist, an\n",
      " |        :class:`HdfsError` will be raised. If it points to a file, the returned\n",
      " |        generator will be empty.\n",
      " |      :param depth: Maximum depth to explore. `0` for no limit.\n",
      " |      :param status: Also return each file or folder's corresponding FileStatus_.\n",
      " |      \n",
      " |      This method returns a generator yielding tuples `(path, dirs, files)`\n",
      " |      where `path` is the absolute path to the current directory, `dirs` is the\n",
      " |      list of directory names it contains, and `files` is the list of file names\n",
      " |      it contains.\n",
      " |  \n",
      " |  write(self, hdfs_path, data=None, overwrite=False, permission=None, blocksize=None, replication=None, buffersize=None, append=False, encoding=None)\n",
      " |      Create a file on HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: Path where to create file. The necessary directories will\n",
      " |        be created appropriately.\n",
      " |      :param data: Contents of file to write. Can be a string, a generator or a\n",
      " |        file object. The last two options will allow streaming upload (i.e.\n",
      " |        without having to load the entire contents into memory). If `None`, this\n",
      " |        method will return a file-like object and should be called using a `with`\n",
      " |        block (see below for examples).\n",
      " |      :param overwrite: Overwrite any existing file or directory.\n",
      " |      :param permission: Octal permission to set on the newly created file.\n",
      " |        Leading zeros may be omitted.\n",
      " |      :param blocksize: Block size of the file.\n",
      " |      :param replication: Number of replications of the file.\n",
      " |      :param buffersize: Size of upload buffer.\n",
      " |      :param append: Append to a file rather than create a new one.\n",
      " |      :param encoding: Encoding used to serialize data written.\n",
      " |      \n",
      " |      Sample usages:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        from json import dump, dumps\n",
      " |      \n",
      " |        records = [\n",
      " |          {'name': 'foo', 'weight': 1},\n",
      " |          {'name': 'bar', 'weight': 2},\n",
      " |        ]\n",
      " |      \n",
      " |        # As a context manager:\n",
      " |        with client.write('data/records.jsonl', encoding='utf-8') as writer:\n",
      " |          dump(records, writer)\n",
      " |      \n",
      " |        # Or, passing in a generator directly:\n",
      " |        client.write('data/records.jsonl', data=dumps(records), encoding='utf-8')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from Client:\n",
      " |  \n",
      " |  from_options(cls, options, class_name='Client') from hdfs.client._ClientType\n",
      " |      Load client from options.\n",
      " |      \n",
      " |      :param options: Options dictionary.\n",
      " |      :param class_name: Client class name. Defaults to the base :class:`Client`\n",
      " |        class.\n",
      " |      \n",
      " |      This method provides a single entry point to instantiate any registered\n",
      " |      :class:`Client` subclass. To register a subclass, simply load its\n",
      " |      containing module. If using the CLI, you can use the `autoload.modules` and\n",
      " |      `autoload.paths` options.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Client:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Client:\n",
      " |  \n",
      " |  __registry__ = {'Client': <class 'hdfs.client.Client'>, 'InsecureClien...\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#  检测hdfsCLI 的包，quickstart 界面是http://hdfscli.readthedocs.io/en/latest/quickstart.html\n",
    "# 需要在~/.hdfscli.cfg 配置不同的节点信息\n",
    "\n",
    "#interactive commend line: hdfscli --alias=dev(在上述配置的文件中所定义的)\n",
    "\n",
    "from hdfs import Config\n",
    "client = Config().get_client('dev')\n",
    "\n",
    "print dir(client)\n",
    "print help(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='i-2o02fenq', port=50075): Max retries exceeded with url: /webhdfs/v1/demo/test2.txt?op=OPEN&user.name=root&namenoderpcaddress=192.168.121.2:8020&offset=0 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f7d0448e610>: Failed to establish a new connection: [Errno -2] Name or service not known',))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f47fd00267da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test2.txt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/hdfs/client.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, hdfs_path, offset, length, buffer_size, encoding, chunk_size, delimiter, progress)\u001b[0m\n\u001b[0;32m    595\u001b[0m       \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m       \u001b[0mbuffersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m     )\n\u001b[0;32m    599\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/hdfs/client.pyc\u001b[0m in \u001b[0;36mapi_handler\u001b[1;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m       )\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/hdfs/client.pyc\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, strict, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m       \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m       \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'content-type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'application/octet-stream'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# For HttpFS.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstrict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Non 2XX status code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    466\u001b[0m         }\n\u001b[0;32m    467\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             )\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    435\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='i-2o02fenq', port=50075): Max retries exceeded with url: /webhdfs/v1/demo/test2.txt?op=OPEN&user.name=root&namenoderpcaddress=192.168.121.2:8020&offset=0 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f7d0448e610>: Failed to establish a new connection: [Errno -2] Name or service not known',))"
     ]
    }
   ],
   "source": [
    "with client.read('test2.txt') as reader:\n",
    "  features = reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NewConnectionError",
     "evalue": "<requests.packages.urllib3.connection.HTTPConnection object at 0x7f7cef562050>: Failed to establish a new connection: [Errno -2] Name or service not known",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-fc3306856237>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/test.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hahahaha'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/hdfs/util.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m     97\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_err\u001b[0m \u001b[1;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m       \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Child terminated without errors.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <requests.packages.urllib3.connection.HTTPConnection object at 0x7f7cef562050>: Failed to establish a new connection: [Errno -2] Name or service not known"
     ]
    }
   ],
   "source": [
    "with client.write('/test.txt',encoding='utf-8') as writer:\n",
    "    writer.write('hahahaha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'directoryCount': 223,\n",
       " u'fileCount': 113600,\n",
       " u'length': 262537341699,\n",
       " u'quota': 9223372036854775807,\n",
       " u'spaceConsumed': 1524481883755,\n",
       " u'spaceQuota': -1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = client.content('/')\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'clarkchen', u'demo', u'spark', u'temp', u'test', u'tmp', u'user']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = client.list('/')\n",
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>使用TokenClient </h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h1>使用TokenClient </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__registry__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_append', '_create', '_delete', '_get_content_summary', '_get_file_checksum', '_get_file_status', '_get_home_directory', '_list_status', '_mkdirs', '_open', '_rename', '_request', '_session', '_set_owner', '_set_permission', '_set_replication', '_set_times', '_timeout', 'checksum', 'content', 'delete', 'download', 'from_options', 'list', 'makedirs', 'parts', 'read', 'rename', 'resolve', 'root', 'set_owner', 'set_permission', 'set_replication', 'set_times', 'status', 'upload', 'url', 'walk', 'write']\n",
      "[u'2016_05_02.user.lines.csv', u'test.txt', u'test2.txt', u'test3.txt', u'tmp']\n",
      "/demo\n",
      "http://192.168.121.2:50070\n",
      "Help on method makedirs in module hdfs.client:\n",
      "\n",
      "makedirs(self, hdfs_path, permission=None) method of hdfs.client.TokenClient instance\n",
      "    Create a remote directory, recursively if necessary.\n",
      "    \n",
      "    :param hdfs_path: Remote path. Intermediate directories will be created\n",
      "      appropriately.\n",
      "    :param permission: Octal permission to set on the newly created directory.\n",
      "      These permissions will only be set on directories that do not already\n",
      "      exist.\n",
      "    \n",
      "    This function currently has no return value as WebHDFS doesn't return a\n",
      "    meaningful flag.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### hdfs 包的debug\n",
    "\n",
    "from hdfs import TokenClient\n",
    "\n",
    "client = TokenClient('http://192.168.121.2:50070', 'tian', root='/demo')\n",
    "\n",
    "# print client\n",
    "print dir(client)\n",
    "\n",
    "# client.content('/')\n",
    "print client.list('/demo')\n",
    "print client.root\n",
    "print client.url\n",
    "print help(client.makedirs)\n",
    "\n",
    "# client.makedirs(client.root+'/demo') #创建文件夹\n",
    "# client.delete('demo', recursive=True) #删除文件夹\n",
    "\n",
    "# client.rename('test.txt','test2.txt') #修改文件名\n",
    "\n",
    "# client.set_permission('test2.txt','777') #permission denied\n",
    "# client.set_permission('demo','777') #可以修改了\n",
    "\n",
    "# client.write('hello.md', 'Hello, world!',permission='777')\n",
    "# client.rename('hello.md', 'hello.rst')\n",
    "# client.download('hello.rst', 'hello.rst')\n",
    "# client.delete('hello.rst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method read in module hdfs.client:\n",
      "\n",
      "read(*args, **kwds) method of hdfs.client.TokenClient instance\n",
      "    Read a file from HDFS.\n",
      "    \n",
      "    :param hdfs_path: HDFS path.\n",
      "    :param offset: Starting byte position.\n",
      "    :param length: Number of bytes to be processed. `None` will read the entire\n",
      "      file.\n",
      "    :param buffer_size: Size of the buffer in bytes used for transferring the\n",
      "      data. Defaults the the value set in the HDFS configuration.\n",
      "    :param encoding: Encoding used to decode the request. By default the raw\n",
      "      data is returned. This is mostly helpful in python 3, for example to\n",
      "      deserialize JSON data (as the decoder expects unicode).\n",
      "    :param chunk_size: If set to a positive number, the context manager will\n",
      "      return a generator yielding every `chunk_size` bytes instead of a\n",
      "      file-like object (unless `delimiter` is also set, see below).\n",
      "    :param delimiter: If set, the context manager will return a generator\n",
      "      yielding each time the delimiter is encountered. This parameter requires\n",
      "      the `encoding` to be specified.\n",
      "    :param progress: Callback function to track progress, called every\n",
      "      `chunk_size` bytes (not available if the chunk size isn't specified). It\n",
      "      will be passed two arguments, the path to the file being uploaded and the\n",
      "      number of bytes transferred so far. On completion, it will be called once\n",
      "      with `-1` as second argument.\n",
      "    \n",
      "    This method must be called using a `with` block:\n",
      "    \n",
      "    .. code-block:: python\n",
      "    \n",
      "      with client.read('foo') as reader:\n",
      "        content = reader.read()\n",
      "    \n",
      "    This ensures that connections are always properly closed.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print help(client.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='i-wll0koir', port=50075): Max retries exceeded with url: /webhdfs/v1/demo/test2.txt?op=OPEN&namenoderpcaddress=192.168.121.2:8020&offset=0 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f3d7486f610>: Failed to establish a new connection: [Errno -2] Name or service not known',))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1ae7e7f8910a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test2.txt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     print reader.read()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/hdfs/client.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, hdfs_path, offset, length, buffer_size, encoding, chunk_size, delimiter, progress)\u001b[0m\n\u001b[0;32m    595\u001b[0m       \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m       \u001b[0mbuffersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m     )\n\u001b[0;32m    599\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/hdfs/client.pyc\u001b[0m in \u001b[0;36mapi_handler\u001b[1;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m       )\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/hdfs/client.pyc\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, strict, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m       \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m       \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'content-type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'application/octet-stream'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# For HttpFS.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstrict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Non 2XX status code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    466\u001b[0m         }\n\u001b[0;32m    467\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             )\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    435\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='i-wll0koir', port=50075): Max retries exceeded with url: /webhdfs/v1/demo/test2.txt?op=OPEN&namenoderpcaddress=192.168.121.2:8020&offset=0 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f3d7486f610>: Failed to establish a new connection: [Errno -2] Name or service not known',))"
     ]
    }
   ],
   "source": [
    "with client.read('test2.txt') as reader:\n",
    "    pass\n",
    "#     print reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TokenClient in module hdfs.client:\n",
      "\n",
      "class TokenClient(Client)\n",
      " |  HDFS web client using Hadoop token delegation security.\n",
      " |  \n",
      " |  :param url: Hostname or IP address of HDFS namenode, prefixed with protocol,\n",
      " |    followed by WebHDFS port on namenode\n",
      " |  :param token: Hadoop delegation token.\n",
      " |  :param \\*\\*kwargs: Keyword arguments passed to the base class' constructor.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TokenClient\n",
      " |      Client\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, url, token, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Client:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  checksum(self, hdfs_path)\n",
      " |      Get a remote file's checksum.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path. Must point to a file.\n",
      " |  \n",
      " |  content(self, hdfs_path, strict=True)\n",
      " |      Get ContentSummary_ for a file or folder on HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path.\n",
      " |      :param strict: If `False`, return `None` rather than raise an exception if\n",
      " |        the path doesn't exist.\n",
      " |      \n",
      " |      .. _ContentSummary: CS_\n",
      " |      .. _CS: http://hadoop.apache.org/docs/r1.0.4/webhdfs.html#ContentSummary\n",
      " |  \n",
      " |  delete(self, hdfs_path, recursive=False)\n",
      " |      Remove a file or directory from HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param recursive: Recursively delete files and directories. By default,\n",
      " |        this method will raise an :class:`HdfsError` if trying to delete a\n",
      " |        non-empty directory.\n",
      " |      \n",
      " |      This function returns `True` if the deletion was successful and `False` if\n",
      " |      no file or directory previously existed at `hdfs_path`.\n",
      " |  \n",
      " |  download(self, hdfs_path, local_path, overwrite=False, n_threads=1, temp_dir=None, **kwargs)\n",
      " |      Download a file or folder from HDFS and save it locally.\n",
      " |      \n",
      " |      :param hdfs_path: Path on HDFS of the file or folder to download. If a\n",
      " |        folder, all the files under it will be downloaded.\n",
      " |      :param local_path: Local path. If it already exists and is a directory,\n",
      " |        the files will be downloaded inside of it.\n",
      " |      :param overwrite: Overwrite any existing file or directory.\n",
      " |      :param n_threads: Number of threads to use for parallelization. A value of\n",
      " |        `0` (or negative) uses as many threads as there are files.\n",
      " |      :param temp_dir: Directory under which the files will first be downloaded\n",
      " |        when `overwrite=True` and the final destination path already exists. Once\n",
      " |        the download successfully completes, it will be swapped in.\n",
      " |      :param \\*\\*kwargs: Keyword arguments forwarded to :meth:`read`. If no\n",
      " |        `chunk_size` argument is passed, a default value of 64 kB will be used.\n",
      " |        If a `progress` argument is passed and threading is used, care must be\n",
      " |        taken to ensure correct behavior.\n",
      " |      \n",
      " |      On success, this method returns the local download path.\n",
      " |  \n",
      " |  list(self, hdfs_path, status=False)\n",
      " |      Return names of files contained in a remote folder.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path to a directory. If `hdfs_path` doesn't exist\n",
      " |        or points to a normal file, an :class:`HdfsError` will be raised.\n",
      " |      :param status: Also return each file's corresponding FileStatus_.\n",
      " |  \n",
      " |  makedirs(self, hdfs_path, permission=None)\n",
      " |      Create a remote directory, recursively if necessary.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path. Intermediate directories will be created\n",
      " |        appropriately.\n",
      " |      :param permission: Octal permission to set on the newly created directory.\n",
      " |        These permissions will only be set on directories that do not already\n",
      " |        exist.\n",
      " |      \n",
      " |      This function currently has no return value as WebHDFS doesn't return a\n",
      " |      meaningful flag.\n",
      " |  \n",
      " |  parts(self, hdfs_path, parts=None, status=False)\n",
      " |      Returns a dictionary of part-files corresponding to a path.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path.\n",
      " |      :param parts: List of part-files numbers or total number of part-files to\n",
      " |        select. If a number, that many partitions will be chosen at random. By\n",
      " |        default all part-files are returned. If `parts` is a list and one of the\n",
      " |        parts is not found or too many samples are demanded, an\n",
      " |        :class:`~hdfs.util.HdfsError` is raised.\n",
      " |      :param status: Also return each file's corresponding FileStatus_.\n",
      " |  \n",
      " |  read(*args, **kwds)\n",
      " |      Read a file from HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param offset: Starting byte position.\n",
      " |      :param length: Number of bytes to be processed. `None` will read the entire\n",
      " |        file.\n",
      " |      :param buffer_size: Size of the buffer in bytes used for transferring the\n",
      " |        data. Defaults the the value set in the HDFS configuration.\n",
      " |      :param encoding: Encoding used to decode the request. By default the raw\n",
      " |        data is returned. This is mostly helpful in python 3, for example to\n",
      " |        deserialize JSON data (as the decoder expects unicode).\n",
      " |      :param chunk_size: If set to a positive number, the context manager will\n",
      " |        return a generator yielding every `chunk_size` bytes instead of a\n",
      " |        file-like object (unless `delimiter` is also set, see below).\n",
      " |      :param delimiter: If set, the context manager will return a generator\n",
      " |        yielding each time the delimiter is encountered. This parameter requires\n",
      " |        the `encoding` to be specified.\n",
      " |      :param progress: Callback function to track progress, called every\n",
      " |        `chunk_size` bytes (not available if the chunk size isn't specified). It\n",
      " |        will be passed two arguments, the path to the file being uploaded and the\n",
      " |        number of bytes transferred so far. On completion, it will be called once\n",
      " |        with `-1` as second argument.\n",
      " |      \n",
      " |      This method must be called using a `with` block:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        with client.read('foo') as reader:\n",
      " |          content = reader.read()\n",
      " |      \n",
      " |      This ensures that connections are always properly closed.\n",
      " |  \n",
      " |  rename(self, hdfs_src_path, hdfs_dst_path)\n",
      " |      Move a file or folder.\n",
      " |      \n",
      " |      :param hdfs_src_path: Source path.\n",
      " |      :param hdfs_dst_path: Destination path. If the path already exists and is\n",
      " |        a directory, the source will be moved into it. If the path exists and is\n",
      " |        a file, or if a parent destination directory is missing, this method will\n",
      " |        raise an :class:`HdfsError`.\n",
      " |  \n",
      " |  resolve(self, hdfs_path)\n",
      " |      Return absolute, normalized path, with special markers expanded.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path.\n",
      " |      \n",
      " |      Currently supported markers:\n",
      " |      \n",
      " |      * `'#LATEST'`: this marker gets expanded to the most recently updated file\n",
      " |        or folder. They can be combined using the `'{N}'` suffix. For example,\n",
      " |        `'foo/#LATEST{2}'` is equivalent to `'foo/#LATEST/#LATEST'`.\n",
      " |  \n",
      " |  set_owner(self, hdfs_path, owner=None, group=None)\n",
      " |      Change the owner of file.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param owner: Optional, new owner for file.\n",
      " |      :param group: Optional, new group for file.\n",
      " |      \n",
      " |      At least one of `owner` and `group` must be specified.\n",
      " |  \n",
      " |  set_permission(self, hdfs_path, permission)\n",
      " |      Change the permissions of file.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param permission: New octal permissions string of file.\n",
      " |  \n",
      " |  set_replication(self, hdfs_path, replication)\n",
      " |      Set file replication.\n",
      " |      \n",
      " |      :param hdfs_path: Path to an existing remote file. An :class:`HdfsError`\n",
      " |        will be raised if the path doesn't exist or points to a directory.\n",
      " |      :param replication: Replication factor.\n",
      " |  \n",
      " |  set_times(self, hdfs_path, access_time=None, modification_time=None)\n",
      " |      Change remote timestamps.\n",
      " |      \n",
      " |      :param hdfs_path: HDFS path.\n",
      " |      :param access_time: Timestamp of last file access.\n",
      " |      :param modification_time: Timestamps of last file access.\n",
      " |  \n",
      " |  status(self, hdfs_path, strict=True)\n",
      " |      Get FileStatus_ for a file or folder on HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: Remote path.\n",
      " |      :param strict: If `False`, return `None` rather than raise an exception if\n",
      " |        the path doesn't exist.\n",
      " |      \n",
      " |      .. _FileStatus: FS_\n",
      " |      .. _FS: http://hadoop.apache.org/docs/r1.0.4/webhdfs.html#FileStatus\n",
      " |  \n",
      " |  upload(self, hdfs_path, local_path, overwrite=False, n_threads=1, temp_dir=None, chunk_size=65536, progress=None, cleanup=True, **kwargs)\n",
      " |      Upload a file or directory to HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: Target HDFS path. If it already exists and is a\n",
      " |        directory, files will be uploaded inside.\n",
      " |      :param local_path: Local path to file or folder. If a folder, all the files\n",
      " |        inside of it will be uploaded (note that this implies that folders empty\n",
      " |        of files will not be created remotely).\n",
      " |      :param overwrite: Overwrite any existing file or directory.\n",
      " |      :param n_threads: Number of threads to use for parallelization. A value of\n",
      " |        `0` (or negative) uses as many threads as there are files.\n",
      " |      :param temp_dir: Directory under which the files will first be uploaded\n",
      " |        when `overwrite=True` and the final remote path already exists. Once the\n",
      " |        upload successfully completes, it will be swapped in.\n",
      " |      :param chunk_size: Interval in bytes by which the files will be uploaded.\n",
      " |      :param progress: Callback function to track progress, called every\n",
      " |        `chunk_size` bytes. It will be passed two arguments, the path to the\n",
      " |        file being uploaded and the number of bytes transferred so far. On\n",
      " |        completion, it will be called once with `-1` as second argument.\n",
      " |      :param cleanup: Delete any uploaded files if an error occurs during the\n",
      " |        upload.\n",
      " |      :param \\*\\*kwargs: Keyword arguments forwarded to :meth:`write`.\n",
      " |      \n",
      " |      On success, this method returns the remote upload path.\n",
      " |  \n",
      " |  walk(self, hdfs_path, depth=0, status=False)\n",
      " |      Depth-first walk of remote filesystem.\n",
      " |      \n",
      " |      :param hdfs_path: Starting path. If the path doesn't exist, an\n",
      " |        :class:`HdfsError` will be raised. If it points to a file, the returned\n",
      " |        generator will be empty.\n",
      " |      :param depth: Maximum depth to explore. `0` for no limit.\n",
      " |      :param status: Also return each file or folder's corresponding FileStatus_.\n",
      " |      \n",
      " |      This method returns a generator yielding tuples `(path, dirs, files)`\n",
      " |      where `path` is the absolute path to the current directory, `dirs` is the\n",
      " |      list of directory names it contains, and `files` is the list of file names\n",
      " |      it contains.\n",
      " |  \n",
      " |  write(self, hdfs_path, data=None, overwrite=False, permission=None, blocksize=None, replication=None, buffersize=None, append=False, encoding=None)\n",
      " |      Create a file on HDFS.\n",
      " |      \n",
      " |      :param hdfs_path: Path where to create file. The necessary directories will\n",
      " |        be created appropriately.\n",
      " |      :param data: Contents of file to write. Can be a string, a generator or a\n",
      " |        file object. The last two options will allow streaming upload (i.e.\n",
      " |        without having to load the entire contents into memory). If `None`, this\n",
      " |        method will return a file-like object and should be called using a `with`\n",
      " |        block (see below for examples).\n",
      " |      :param overwrite: Overwrite any existing file or directory.\n",
      " |      :param permission: Octal permission to set on the newly created file.\n",
      " |        Leading zeros may be omitted.\n",
      " |      :param blocksize: Block size of the file.\n",
      " |      :param replication: Number of replications of the file.\n",
      " |      :param buffersize: Size of upload buffer.\n",
      " |      :param append: Append to a file rather than create a new one.\n",
      " |      :param encoding: Encoding used to serialize data written.\n",
      " |      \n",
      " |      Sample usages:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        from json import dump, dumps\n",
      " |      \n",
      " |        records = [\n",
      " |          {'name': 'foo', 'weight': 1},\n",
      " |          {'name': 'bar', 'weight': 2},\n",
      " |        ]\n",
      " |      \n",
      " |        # As a context manager:\n",
      " |        with client.write('data/records.jsonl', encoding='utf-8') as writer:\n",
      " |          dump(records, writer)\n",
      " |      \n",
      " |        # Or, passing in a generator directly:\n",
      " |        client.write('data/records.jsonl', data=dumps(records), encoding='utf-8')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from Client:\n",
      " |  \n",
      " |  from_options(cls, options, class_name='Client') from hdfs.client._ClientType\n",
      " |      Load client from options.\n",
      " |      \n",
      " |      :param options: Options dictionary.\n",
      " |      :param class_name: Client class name. Defaults to the base :class:`Client`\n",
      " |        class.\n",
      " |      \n",
      " |      This method provides a single entry point to instantiate any registered\n",
      " |      :class:`Client` subclass. To register a subclass, simply load its\n",
      " |      containing module. If using the CLI, you can use the `autoload.modules` and\n",
      " |      `autoload.paths` options.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Client:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Client:\n",
      " |  \n",
      " |  __registry__ = {'Client': <class 'hdfs.client.Client'>, 'InsecureClien...\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(TokenClient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method set_permission in module hdfs.client:\n",
      "\n",
      "set_permission(self, hdfs_path, permission) unbound hdfs.client.TokenClient method\n",
      "    Change the permissions of file.\n",
      "    \n",
      "    :param hdfs_path: HDFS path.\n",
      "    :param permission: New octal permissions string of file.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(TokenClient.set_permission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'libhdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3c68e7df093f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibhdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'libhdfs' is not defined"
     ]
    }
   ],
   "source": [
    "help(libhdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>pydoop 包的调试</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h1>pydoop 包的调试</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pydoop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-dca4ca033cd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpydoop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named pydoop"
     ]
    }
   ],
   "source": [
    "import pydoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>snakebite 包的调试</h1>\n",
       "<a href=\"https://github.com/spotify/snakebite\">git</a>\n",
       "<a href=\"http://snakebite.readthedocs.io/en/latest/cli.html\">doc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h1>snakebite 包的调试</h1>\n",
    "<a href=\"https://github.com/spotify/snakebite\">git</a>\n",
    "<a href=\"http://snakebite.readthedocs.io/en/latest/cli.html\">doc</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>普通的snakebite.client 对象好像不能Push文件到hdfs</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%%html\n",
    "<h3>普通的snakebite.client 对象好像不能Push文件到hdfs</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import snakebite\n",
    "from snakebite.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = Client(\"192.168.121.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'used': 1538832080896L, 'capacity': 3698182856704L, 'under_replicated': 0L, 'missing_blocks': 0L, 'filesystem': 'hdfs://192.168.121.2:8020', 'remaining': 1881487945728L, 'corrupt_blocks': 0L}\n",
      "{'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'access_time': 1465279762709L, 'block_replication': 3, 'modification_time': 1465279762711L, 'length': 0L, 'blocksize': 134217728L, 'owner': u'root', 'path': '/demo/test.txt'}\n",
      "{'group': u'supergroup', 'permission': 755, 'file_type': 'f', 'access_time': 1465280376911L, 'block_replication': 3, 'modification_time': 1464866093643L, 'length': 4L, 'blocksize': 134217728L, 'owner': u'root', 'path': '/demo/test2.txt'}\n",
      "{'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'access_time': 1465279825867L, 'block_replication': 3, 'modification_time': 1465279825872L, 'length': 0L, 'blocksize': 134217728L, 'owner': u'root', 'path': '/demo/test3.txt'}\n",
      "{'group': u'supergroup', 'permission': 493, 'file_type': 'd', 'access_time': 0L, 'block_replication': 0, 'modification_time': 1465280313135L, 'length': 0L, 'blocksize': 0L, 'owner': u'root', 'path': '/demo/tmp'}\n"
     ]
    }
   ],
   "source": [
    "client\n",
    "print client.df() #Get FS information\n",
    "\n",
    "for i in client.ls(['/demo']):\n",
    "    print i\n",
    "    \n",
    "# for i in client.ls(['/demo']):\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '/demo/test2.txt', 'result': True}\n"
     ]
    }
   ],
   "source": [
    "#chmod\n",
    "for i in client.chmod(['/demo/test2.txt'],755):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '/demo/test.txt', 'result': False, 'error': 'File already exists'}\n",
      "{'path': '/demo/test3.txt', 'result': True}\n"
     ]
    }
   ],
   "source": [
    "#创建文件 touchz函数 -- 如果有该文件存在，报错‘file alreay exists’\n",
    "for i in client.touchz(['/demo/test.txt','/demo/test3.txt']):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '/demo/tmp', 'result': False, 'error': \"mkdir: `/demo/tmp': File exists\"}\n",
      "{'path': '/root/test2.txt', 'source_path': '/demo/test2.txt', 'result': True, 'error': ''}\n",
      "{'path': '/root/test.txt', 'source_path': '/demo/test.txt', 'result': False, 'error': 'file exists'}\n"
     ]
    }
   ],
   "source": [
    "# for i in client.mkdir(['/demo/tmp']):\n",
    "#     print i\n",
    "#copyToLocal() 函数. 第一个参数是hdfs中file的列表， 后面是本机的目录 或 文件， 同样如果文件存在，返回的\n",
    "# dict里会包括error 信息的\n",
    "for i in client.copyToLocal(['/demo/test2.txt','/demo/test.txt'],'/root'):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 读取文件 test2.txt --- client.text\n",
    "print \n",
    "for i in client.text(['/demo/test2.txt']):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8020"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put 文件还没有："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method getmerge in module snakebite.client:\n",
      "\n",
      "getmerge(self, path, dst, newline=False, check_crc=False) method of snakebite.client.Client instance\n",
      "    Get all the files in the directories that\n",
      "    match the source file pattern and merge and sort them to only\n",
      "    one file on local fs.\n",
      "    \n",
      "    :param paths: Directory containing files that will be merged\n",
      "    :type paths: string\n",
      "    :param dst: Path of file that will be written\n",
      "    :type dst: string\n",
      "    :param nl: Add a newline character at the end of each file.\n",
      "    :type nl: boolean\n",
      "    :returns: string content of the merged file at dst\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# merge 数据， 写入数据\n",
    "print help(client.getmerge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _handle_cat at 0x7f7cef575c80>\n"
     ]
    }
   ],
   "source": [
    "for i in client.cat(['/demo/test2.txt']):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>采用minicluster 类</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h3>采用minicluster 类</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snakebite.minicluster import MiniCluster\n",
    "from snakebite.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('16/06/07 14:55:16 INFO mapreduce.MiniHadoopClusterManager: Updated 0 configuration settings from command line.\\n',)\n",
      "('16/06/07 14:55:17 INFO hdfs.MiniDFSCluster: starting cluster: numNameNodes=1, numDataNodes=1\\n',)\n",
      "('Formatting using clusterid: testClusterID\\n',)\n",
      "('16/06/07 14:55:18 INFO namenode.FSNamesystem: No KeyProvider found.\\n',)\n",
      "('16/06/07 14:55:18 INFO namenode.FSNamesystem: fsLock is fair:true\\n',)\n",
      "('16/06/07 14:55:18 INFO Configuration.deprecation: hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping\\n',)\n",
      "('16/06/07 14:55:18 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\\n',)\n",
      "('16/06/07 14:55:18 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\\n',)\n",
      "('16/06/07 14:55:18 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\\n',)\n",
      "('16/06/07 14:55:18 INFO blockmanagement.BlockManager: The block deletion will start around 2016 Jun 07 14:55:18\\n',)\n",
      "('16/06/07 14:55:18 INFO util.GSet: Computing capacity for map BlocksMap\\n',)\n",
      "('16/06/07 14:55:18 INFO util.GSet: VM type       = 64-bit\\n',)\n",
      "('16/06/07 14:55:18 INFO util.GSet: 2.0% max memory 494.9 MB = 9.9 MB\\n',)\n",
      "('16/06/07 14:55:18 INFO util.GSet: capacity      = 2^20 = 1048576 entries\\n',)\n",
      "('16/06/07 14:55:19 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\\n',)\n",
      "('16/06/07 14:55:19 INFO blockmanagement.BlockManager: defaultReplication         = 1\\n',)\n",
      "('16/06/07 14:55:19 INFO blockmanagement.BlockManager: maxReplication             = 512\\n',)\n",
      "('16/06/07 14:55:19 INFO blockmanagement.BlockManager: minReplication             = 1\\n',)\n",
      "('16/06/07 14:55:19 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\\n',)\n",
      "('16/06/07 14:55:19 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\\n',)\n",
      "('16/06/07 14:55:19 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\\n',)\n",
      "('16/06/07 14:55:19 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: supergroup          = supergroup\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: isPermissionEnabled = true\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: HA Enabled: false\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: Append Enabled: true\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: Computing capacity for map INodeMap\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: VM type       = 64-bit\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: 1.0% max memory 494.9 MB = 4.9 MB\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: capacity      = 2^19 = 524288 entries\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSDirectory: ACLs enabled? false\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSDirectory: XAttrs enabled? true\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSDirectory: Maximum size of an xattr: 16384\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.NameNode: Caching file names occuring more than 10 times\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: Computing capacity for map cachedBlocks\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: VM type       = 64-bit\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: 0.25% max memory 494.9 MB = 1.2 MB\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: capacity      = 2^17 = 131072 entries\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 0\\n',)\n",
      "('16/06/07 14:55:19 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\\n',)\n",
      "('16/06/07 14:55:19 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\\n',)\n",
      "('16/06/07 14:55:19 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: Computing capacity for map NameNodeRetryCache\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: VM type       = 64-bit\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: 0.029999999329447746% max memory 494.9 MB = 152.0 KB\\n',)\n",
      "('16/06/07 14:55:19 INFO util.GSet: capacity      = 2^14 = 16384 entries\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.FSImage: Allocated new BlockPoolId: BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:19 INFO common.Storage: Storage directory /root/build/test/data/dfs/name1 has been successfully formatted.\\n',)\n",
      "('16/06/07 14:55:19 INFO common.Storage: Storage directory /root/build/test/data/dfs/name2 has been successfully formatted.\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.NameNode: createNameNode []\\n',)\n",
      "('16/06/07 14:55:19 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\\n',)\n",
      "('16/06/07 14:55:19 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\\n',)\n",
      "('16/06/07 14:55:19 INFO impl.MetricsSystemImpl: NameNode metrics system started\\n',)\n",
      "('16/06/07 14:55:19 INFO namenode.NameNode: fs.defaultFS is hdfs://127.0.0.1:0\\n',)\n",
      "('16/06/07 14:55:20 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://localhost:0\\n',)\n",
      "('16/06/07 14:55:20 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\\n',)\n",
      "('16/06/07 14:55:20 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\\n',)\n",
      "('16/06/07 14:55:20 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\\n',)\n",
      "(\"16/06/07 14:55:20 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\\n\",)\n",
      "('16/06/07 14:55:20 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\\n',)\n",
      "('16/06/07 14:55:20 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\\n',)\n",
      "('16/06/07 14:55:20 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\\n',)\n",
      "(\"16/06/07 14:55:20 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\\n\",)\n",
      "('16/06/07 14:55:20 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\\n',)\n",
      "('16/06/07 14:55:20 INFO http.HttpServer2: Jetty bound to port 42321\\n',)\n",
      "('16/06/07 14:55:20 INFO mortbay.log: jetty-6.1.26\\n',)\n",
      "('16/06/07 14:55:20 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42321\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: No KeyProvider found.\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: fsLock is fair:true\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: The block deletion will start around 2016 Jun 07 14:55:20\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: Computing capacity for map BlocksMap\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: VM type       = 64-bit\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: 2.0% max memory 494.9 MB = 9.9 MB\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: capacity      = 2^20 = 1048576 entries\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: defaultReplication         = 1\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: maxReplication             = 512\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: minReplication             = 1\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\\n',)\n",
      "('16/06/07 14:55:20 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: supergroup          = supergroup\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: isPermissionEnabled = true\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: HA Enabled: false\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: Append Enabled: true\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: Computing capacity for map INodeMap\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: VM type       = 64-bit\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: 1.0% max memory 494.9 MB = 4.9 MB\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: capacity      = 2^19 = 524288 entries\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSDirectory: ACLs enabled? false\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSDirectory: XAttrs enabled? true\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSDirectory: Maximum size of an xattr: 16384\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.NameNode: Caching file names occuring more than 10 times\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: Computing capacity for map cachedBlocks\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: VM type       = 64-bit\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: 0.25% max memory 494.9 MB = 1.2 MB\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: capacity      = 2^17 = 131072 entries\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 0\\n',)\n",
      "('16/06/07 14:55:20 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\\n',)\n",
      "('16/06/07 14:55:20 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\\n',)\n",
      "('16/06/07 14:55:20 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: Computing capacity for map NameNodeRetryCache\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: VM type       = 64-bit\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: 0.029999999329447746% max memory 494.9 MB = 152.0 KB\\n',)\n",
      "('16/06/07 14:55:20 INFO util.GSet: capacity      = 2^14 = 16384 entries\\n',)\n",
      "('16/06/07 14:55:20 INFO common.Storage: Lock on /root/build/test/data/dfs/name1/in_use.lock acquired by nodename 17662@i-w36gsfjh\\n',)\n",
      "('16/06/07 14:55:20 INFO common.Storage: Lock on /root/build/test/data/dfs/name2/in_use.lock acquired by nodename 17662@i-w36gsfjh\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FileJournalManager: Recovering unfinalized segments in /root/build/test/data/dfs/name1/current\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FileJournalManager: Recovering unfinalized segments in /root/build/test/data/dfs/name2/current\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSImage: No edit log streams selected.\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSImage: Loaded image for txid 0 from /root/build/test/data/dfs/name1/current/fsimage_0000000000000000000\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSEditLog: Starting log segment at 1\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.NameCache: initialized with 0 entries 0 lookups\\n',)\n",
      "('16/06/07 14:55:20 INFO namenode.FSNamesystem: Finished loading FSImage in 118 msecs\\n',)\n",
      "('16/06/07 14:55:21 INFO namenode.NameNode: RPC server is binding to localhost:0\\n',)\n",
      "('16/06/07 14:55:21 INFO ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\\n',)\n",
      "('16/06/07 14:55:21 INFO ipc.Server: Starting Socket Reader #1 for port 41809\\n',)\n",
      "('16/06/07 14:55:21 INFO namenode.NameNode: Clients are to use localhost:41809 to access this namenode/service.\\n',)\n",
      "('16/06/07 14:55:21 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean\\n',)\n",
      "('16/06/07 14:55:21 INFO namenode.LeaseManager: Number of blocks under construction: 0\\n',)\n",
      "('16/06/07 14:55:21 INFO namenode.LeaseManager: Number of blocks under construction: 0\\n',)\n",
      "('16/06/07 14:55:21 INFO namenode.FSNamesystem: initializing replication queues\\n',)\n",
      "('16/06/07 14:55:21 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\\n',)\n",
      "('16/06/07 14:55:21 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\\n',)\n",
      "('16/06/07 14:55:21 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\\n',)\n",
      "('16/06/07 14:55:21 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0\\n',)\n",
      "('16/06/07 14:55:21 INFO blockmanagement.BlockManager: Total number of blocks            = 0\\n',)\n",
      "('16/06/07 14:55:21 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\\n',)\n",
      "('16/06/07 14:55:21 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\\n',)\n",
      "('16/06/07 14:55:21 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\\n',)\n",
      "('16/06/07 14:55:21 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\\n',)\n",
      "('16/06/07 14:55:21 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 10 msec\\n',)\n",
      "('16/06/07 14:55:21 INFO ipc.Server: IPC Server Responder: starting\\n',)\n",
      "('16/06/07 14:55:21 INFO ipc.Server: IPC Server listener on 41809: starting\\n',)\n",
      "('16/06/07 14:55:21 INFO namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:41809\\n',)\n",
      "('16/06/07 14:55:21 INFO namenode.FSNamesystem: Starting services required for active state\\n',)\n",
      "('16/06/07 14:55:21 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\\n',)\n",
      "('16/06/07 14:55:21 INFO hdfs.MiniDFSCluster: Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/build/test/data/dfs/data/data1,[DISK]file:/root/build/test/data/dfs/data/data2\\n',)\n",
      "('16/06/07 14:55:21 INFO impl.MetricsSystemImpl: DataNode metrics system started (again)\\n',)\n",
      "('16/06/07 14:55:21 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\\n',)\n",
      "('16/06/07 14:55:21 INFO datanode.DataNode: Configured hostname is 127.0.0.1\\n',)\n",
      "('16/06/07 14:55:21 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\\n',)\n",
      "('16/06/07 14:55:21 INFO datanode.DataNode: Opened streaming server at /127.0.0.1:43988\\n',)\n",
      "('16/06/07 14:55:21 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s\\n',)\n",
      "('16/06/07 14:55:21 INFO datanode.DataNode: Number threads for balancing is 5\\n',)\n",
      "('16/06/07 14:55:21 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\\n',)\n",
      "('16/06/07 14:55:21 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\\n',)\n",
      "(\"16/06/07 14:55:21 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\\n\",)\n",
      "('16/06/07 14:55:21 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\\n',)\n",
      "('16/06/07 14:55:21 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\\n',)\n",
      "('16/06/07 14:55:21 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\\n',)\n",
      "('16/06/07 14:55:21 INFO http.HttpServer2: Jetty bound to port 34490\\n',)\n",
      "('16/06/07 14:55:21 INFO mortbay.log: jetty-6.1.26\\n',)\n",
      "('16/06/07 14:55:21 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34490\\n',)\n",
      "('16/06/07 14:55:22 INFO web.DatanodeHttpServer: Listening HTTP traffic on /127.0.0.1:34357\\n',)\n",
      "('16/06/07 14:55:22 INFO datanode.DataNode: dnUserName = root\\n',)\n",
      "('16/06/07 14:55:22 INFO datanode.DataNode: supergroup = supergroup\\n',)\n",
      "('16/06/07 14:55:22 INFO ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\\n',)\n",
      "('16/06/07 14:55:22 INFO ipc.Server: Starting Socket Reader #1 for port 34301\\n',)\n",
      "('16/06/07 14:55:22 INFO datanode.DataNode: Opened IPC server at /127.0.0.1:34301\\n',)\n",
      "('16/06/07 14:55:22 INFO datanode.DataNode: Refresh request received for nameservices: null\\n',)\n",
      "('16/06/07 14:55:22 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\\n',)\n",
      "('16/06/07 14:55:22 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:41809 starting to offer service\\n',)\n",
      "('16/06/07 14:55:22 INFO ipc.Server: IPC Server Responder: starting\\n',)\n",
      "('16/06/07 14:55:22 INFO ipc.Server: IPC Server listener on 34301: starting\\n',)\n",
      "('16/06/07 14:55:22 INFO common.Storage: Lock on /root/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 17662@i-w36gsfjh\\n',)\n",
      "('16/06/07 14:55:22 INFO common.Storage: Storage directory /root/build/test/data/dfs/data/data1 is not formatted for BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:22 INFO common.Storage: Formatting ...\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Analyzing storage directories for bpid BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Locking is disabled for /root/build/test/data/dfs/data/data1/current/BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Block pool storage directory /root/build/test/data/dfs/data/data1/current/BP-2140501214-127.0.1.1-1465282519455 is not formatted for BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Formatting ...\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Formatting block pool BP-2140501214-127.0.1.1-1465282519455 directory /root/build/test/data/dfs/data/data1/current/BP-2140501214-127.0.1.1-1465282519455/current\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Lock on /root/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 17662@i-w36gsfjh\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Storage directory /root/build/test/data/dfs/data/data2 is not formatted for BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Formatting ...\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Analyzing storage directories for bpid BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Locking is disabled for /root/build/test/data/dfs/data/data2/current/BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Block pool storage directory /root/build/test/data/dfs/data/data2/current/BP-2140501214-127.0.1.1-1465282519455 is not formatted for BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Formatting ...\\n',)\n",
      "('16/06/07 14:55:23 INFO common.Storage: Formatting block pool BP-2140501214-127.0.1.1-1465282519455 directory /root/build/test/data/dfs/data/data2/current/BP-2140501214-127.0.1.1-1465282519455/current\\n',)\n",
      "('16/06/07 14:55:23 INFO datanode.DataNode: Setting up storage: nsid=452836573;bpid=BP-2140501214-127.0.1.1-1465282519455;lv=-56;nsInfo=lv=-63;cid=testClusterID;nsid=452836573;c=0;bpid=BP-2140501214-127.0.1.1-1465282519455;dnuuid=null\\n',)\n",
      "('16/06/07 14:55:23 INFO datanode.DataNode: Generated and persisted new Datanode UUID b7e4676a-884f-4818-9153-e69cb49c28dc\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Added new volume: DS-57c97c6a-04bb-4e1b-9b21-22154dd1814d\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Added volume - /root/build/test/data/dfs/data/data1/current, StorageType: DISK\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Added new volume: DS-ea34037e-ca2c-4e48-a040-64647a444053\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Added volume - /root/build/test/data/dfs/data/data2/current, StorageType: DISK\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Adding block pool BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Scanning block pool BP-2140501214-127.0.1.1-1465282519455 on volume /root/build/test/data/dfs/data/data1/current...\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Scanning block pool BP-2140501214-127.0.1.1-1465282519455 on volume /root/build/test/data/dfs/data/data2/current...\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-2140501214-127.0.1.1-1465282519455 on /root/build/test/data/dfs/data/data2/current: 48ms\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-2140501214-127.0.1.1-1465282519455 on /root/build/test/data/dfs/data/data1/current: 54ms\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-2140501214-127.0.1.1-1465282519455: 65ms\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-2140501214-127.0.1.1-1465282519455 on volume /root/build/test/data/dfs/data/data1/current...\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-2140501214-127.0.1.1-1465282519455 on volume /root/build/test/data/dfs/data/data1/current: 0ms\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-2140501214-127.0.1.1-1465282519455 on volume /root/build/test/data/dfs/data/data2/current...\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-2140501214-127.0.1.1-1465282519455 on volume /root/build/test/data/dfs/data/data2/current: 0ms\\n',)\n",
      "('16/06/07 14:55:23 INFO impl.FsDatasetImpl: Total time to add all replicas to map: 14ms\\n',)\n",
      "('16/06/07 14:55:23 INFO hdfs.MiniDFSCluster: dnInfo.length != numDataNodes\\n',)\n",
      "('16/06/07 14:55:23 INFO hdfs.MiniDFSCluster: Waiting for cluster to become active\\n',)\n",
      "('16/06/07 14:55:24 INFO hdfs.MiniDFSCluster: dnInfo.length != numDataNodes\\n',)\n",
      "('16/06/07 14:55:24 INFO hdfs.MiniDFSCluster: Waiting for cluster to become active\\n',)\n",
      "('16/06/07 14:55:24 INFO hdfs.MiniDFSCluster: dnInfo.length != numDataNodes\\n',)\n",
      "('16/06/07 14:55:24 INFO hdfs.MiniDFSCluster: Waiting for cluster to become active\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.VolumeScanner: Now scanning bpid BP-2140501214-127.0.1.1-1465282519455 on volume /root/build/test/data/dfs/data/data1\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.VolumeScanner: VolumeScanner(/root/build/test/data/dfs/data/data1, DS-57c97c6a-04bb-4e1b-9b21-22154dd1814d): finished scanning block pool BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.VolumeScanner: Now scanning bpid BP-2140501214-127.0.1.1-1465282519455 on volume /root/build/test/data/dfs/data/data2\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.VolumeScanner: VolumeScanner(/root/build/test/data/dfs/data/data2, DS-ea34037e-ca2c-4e48-a040-64647a444053): finished scanning block pool BP-2140501214-127.0.1.1-1465282519455\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1465287728200 with interval 21600000\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.DataNode: Block pool BP-2140501214-127.0.1.1-1465282519455 (Datanode Uuid null) service to localhost/127.0.0.1:41809 beginning handshake with NN\\n',)\n",
      "('16/06/07 14:55:24 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43988, datanodeUuid=b7e4676a-884f-4818-9153-e69cb49c28dc, infoPort=34357, infoSecurePort=0, ipcPort=34301, storageInfo=lv=-56;cid=testClusterID;nsid=452836573;c=0) storage b7e4676a-884f-4818-9153-e69cb49c28dc\\n',)\n",
      "('16/06/07 14:55:24 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0\\n',)\n",
      "('16/06/07 14:55:24 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:43988\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.DataNode: Block pool Block pool BP-2140501214-127.0.1.1-1465282519455 (Datanode Uuid null) service to localhost/127.0.0.1:41809 successfully registered with NN\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.DataNode: For namenode localhost/127.0.0.1:41809 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\\n',)\n",
      "('16/06/07 14:55:24 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0\\n',)\n",
      "('16/06/07 14:55:24 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-57c97c6a-04bb-4e1b-9b21-22154dd1814d for DN 127.0.0.1:43988\\n',)\n",
      "('16/06/07 14:55:24 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-ea34037e-ca2c-4e48-a040-64647a444053 for DN 127.0.0.1:43988\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.VolumeScanner: VolumeScanner(/root/build/test/data/dfs/data/data1, DS-57c97c6a-04bb-4e1b-9b21-22154dd1814d): no suitable block pools found to scan.  Waiting 1814399901 ms.\\n',)\n",
      "('16/06/07 14:55:24 INFO hdfs.MiniDFSCluster: Cluster is active\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.VolumeScanner: VolumeScanner(/root/build/test/data/dfs/data/data2, DS-ea34037e-ca2c-4e48-a040-64647a444053): no suitable block pools found to scan.  Waiting 1814399894 ms.\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.DataNode: Namenode Block pool BP-2140501214-127.0.1.1-1465282519455 (Datanode Uuid b7e4676a-884f-4818-9153-e69cb49c28dc) service to localhost/127.0.0.1:41809 trying to claim ACTIVE state with txid=1\\n',)\n",
      "('16/06/07 14:55:24 INFO datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-2140501214-127.0.1.1-1465282519455 (Datanode Uuid b7e4676a-884f-4818-9153-e69cb49c28dc) service to localhost/127.0.0.1:41809\\n',)\n",
      "('16/06/07 14:55:24 INFO mapreduce.MiniHadoopClusterManager: Started MiniDFSCluster -- namenode on port 41809\\n',)\n"
     ]
    }
   ],
   "source": [
    "cluster = MiniCluster(\"/data/reg/simple/\")\n",
    "# client = Client('192.168.121.2', cluster.port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in cluster.ls([\"/demo\"]):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in cluster.put('2016_05_02.user.lines.csv','/demo'):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Hadoopy 的debugger</h1>\n",
       "<a href = 'http://snakebite.readthedocs.io/en/latest/client.html'>doc</a><br>\n",
       "hadoopy 包括mapreduce 和 hdfs 的相关接口，有一些dependency"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h1>Hadoopy 的debugger</h1>\n",
    "<a href = 'http://snakebite.readthedocs.io/en/latest/client.html'>doc</a><br>\n",
    "hadoopy 包括mapreduce 和 hdfs 的相关接口，有一些dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named hadoopy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-43c7c7265a9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhadoopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named hadoopy"
     ]
    }
   ],
   "source": [
    "import hadoopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>使用httpfs来put 和 read, httpfs_utils 的包的debug</h1>\n",
       "<a href = 'http://code.jasonbhill.com/python/quickly-putting-files-into-hdfs/'>doc and examples</a><br>\n",
       "<a href  = 'https://github.com/hilljb/httpfs_utils'>git</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h1>使用httpfs来put 和 read, httpfs_utils 的包的debug</h1>\n",
    "<a href = 'http://code.jasonbhill.com/python/quickly-putting-files-into-hdfs/'>doc and examples</a><br>\n",
    "<a href  = 'https://github.com/hilljb/httpfs_utils'>git</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#导入下载的httpfs_utils 包\n",
    "import sys\n",
    "sys.path.append(\"/root/util/httpfs_utils/\")\n",
    "import httpfs_utils as httpfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1   root  supergroup    11481262  2016-06-07 08:40:26 /demo/2016_05_02.user.lines.csv\n",
      "-rw-r--r--   3   root  supergroup           0  2016-06-07 06:09:22 /demo/test.txt\n",
      "---x-wxrw--wx   3   root  supergroup           4  2016-06-02 11:14:53 /demo/test2.txt\n",
      "-rw-r--r--   3   root  supergroup           0  2016-06-07 06:10:25 /demo/test3.txt\n",
      "drwxr-xr-x   0   root  supergroup           0  2016-06-07 06:18:33 /demo/tmp\n"
     ]
    }
   ],
   "source": [
    "httpfs.ls(host='192.168.121.2',user='root',hdfs_path='/demo',port = 50070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='i-2o02fenq', port=50075): Max retries exceeded with url: /webhdfs/v1/demo/2016_05_02.user.lines.csv?op=CREATE&user.name=root&namenoderpcaddress=192.168.121.2:8020&overwrite=true&permission=775 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f3d7483e350>: Failed to establish a new connection: [Errno -2] Name or service not known',))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-fe7beed3faa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 与上述的问题一样，无法建立连接\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhttpfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'192.168.121.2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'root'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhdfs_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/demo'\u001b[0m\u001b[1;33m,\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/data/reg/simple/2016_05_02.user.lines.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50070\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/root/util/httpfs_utils/httpfs_utils.pyc\u001b[0m in \u001b[0;36mput\u001b[1;34m(host, user, hdfs_path, filename, port, perms)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[1;31m#files = {'file': open(filename,'rb')}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m     \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mput\u001b[1;34m(url, data, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'put'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    466\u001b[0m         }\n\u001b[0;32m    467\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             )\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    435\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='i-2o02fenq', port=50075): Max retries exceeded with url: /webhdfs/v1/demo/2016_05_02.user.lines.csv?op=CREATE&user.name=root&namenoderpcaddress=192.168.121.2:8020&overwrite=true&permission=775 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f3d7483e350>: Failed to establish a new connection: [Errno -2] Name or service not known',))"
     ]
    }
   ],
   "source": [
    "# 与上述的问题一样，无法建立连接\n",
    "httpfs.put(host='192.168.121.2',user='root',hdfs_path='/demo',\\\n",
    "           filename='/data/reg/simple/2016_05_02.user.lines.csv',port=50070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='i-x0lguwmt', port=50075): Max retries exceeded with url: /webhdfs/v1/demo/test2.txt?op=OPEN&user.name=tian&namenoderpcaddress=192.168.121.2:8020&offset=0 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f3d74920090>: Failed to establish a new connection: [Errno -2] Name or service not known',))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3f8d79c6005e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhttpfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'192.168.121.2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tian'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/demo/test2.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50070\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/root/util/httpfs_utils/httpfs_utils.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(host, user, hdfs_path, port)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;31m# Form and issue the request.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m     \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    466\u001b[0m         }\n\u001b[0;32m    467\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             )\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    435\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='i-x0lguwmt', port=50075): Max retries exceeded with url: /webhdfs/v1/demo/test2.txt?op=OPEN&user.name=tian&namenoderpcaddress=192.168.121.2:8020&offset=0 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f3d74920090>: Failed to establish a new connection: [Errno -2] Name or service not known',))"
     ]
    }
   ],
   "source": [
    "# 无法 read()文件，仍然是连接不上\n",
    "httpfs.read('192.168.121.2','tian','/demo/test2.txt',port=50070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>pyhdfs 的对hdfs文件的操作debug</h1>\n",
       "<a href='http://www.linuxidc.com/Linux/2013-07/87926.htm'>中文辅助链接</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h1>pyhdfs 的对hdfs文件的操作debug</h1>\n",
    "<a href='http://www.linuxidc.com/Linux/2013-07/87926.htm'>中文辅助链接</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyhdfs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1838383d117d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyhdfs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named pyhdfs"
     ]
    }
   ],
   "source": [
    "import pyhdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>WebHDFS 包的使用</h1>\n",
       "<a href='https://github.com/mk23/webhdfs'>git</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "<h1>WebHDFS 包的使用</h1>\n",
    "<a href='https://github.com/mk23/webhdfs'>git</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
